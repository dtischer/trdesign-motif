#!/software/conda/envs/tensorflow/bin/python
import warnings, logging, os, sys
warnings.filterwarnings('ignore',category=FutureWarning)
logging.disable(logging.WARNING)
os.environ["TF_CPP_MIN_LOG_LEVEL"] = "3"

import sys
import json
import numpy as np
from scipy import stats
import networkx as nx
import argparse, pickle
from itertools import permutations
import tensorflow as tf

from parsers import parse_pdb
from coords6d import get_binned6d
from resnet import network,load_weights
from bsite import probe_bsite_tf_frag
from utils import *

def main():

    config = tf.ConfigProto(
        gpu_options = tf.GPUOptions(allow_growth=True)
    )

    ########################################################
    # 0. process inputs
    ########################################################
    parser = argparse.ArgumentParser(formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument("PDB", type=str, help="input PDB with binding site residues")
    parser.add_argument("DIR", type=str, help="path for saving predictions")
    parser.add_argument('--len', type=int, default=100,  help='sequence length')
    parser.add_argument('--num', type=int, dest='num', default=1,  help='number of models to generate')
    parser.add_argument('--start_num', type=int, dest='start_num', default=0,  help='index of first model filename')
    parser.add_argument('--contigs', type=str, dest='contigs', default=None, help='Regions to constrain, as comma-delimited numeric ranges (e.g. "3-10,24-32")')
    parser.add_argument('--msa_num', type=int, default=1, help='number of hallucinated sequences to be added to the MSA')
    parser.add_argument('--w_sat', type=float, dest='ws', default=1.0, help='weight for the satisfaction term')
    parser.add_argument('--w_consist', type=float, dest='wc', default=1.0, help='weight for the consistency term')
    parser.add_argument('--w_clash', type=float, dest='wcl', default=1.0, help='weight for the clash term')
    #parser.add_argument('--w_cav', type=float, dest='wcav', default=0.1, help='weight for the cavity term')
    parser.add_argument('--drop', type=float, dest='drop', default=0.5, help='dropout rate for the input features')
    parser.add_argument('--step', type=float, dest='step', default=1.0, help='NGD minimization step size')
    parser.add_argument('--beta0', type=float, dest='b0', default=2.0, help='inverse temperature at the beginning of the minimization')
    parser.add_argument('--beta1', type=float, dest='b1', default=20.0, help='inverse temperature at the end of the minimization')
    parser.add_argument('--sample', dest='sample', action='store_true', help='perform NGD+sample')
    parser.add_argument('--opt_iter', type=int, default=100, help='number of minimization steps')
    parser.add_argument('--seq_soft', dest='seq_soft', default=False, action='store_true', help='keep soft sequence representation')
    parser.set_defaults(sample=False)
    args = parser.parse_args()

    # write settings file
    print(vars(args),file=open(args.DIR+'.set','w'))

    # load network parameters
    with open('/home/aivan/for/GyuRie/trRosetta2/training/params.json') as jsonfile:
        params = json.load(jsonfile)

    # parse binding site
    bsite_xyz,bsite_idx = parse_pdb(args.PDB,args.contigs)
    
    bsite = get_binned6d(bsite_xyz)
    bsite_nres  = bsite_idx.shape[0]
    
    # calculate number of continuous fragments in the bs
    # based on continuity of residue indices read from .pdb
    bsite_nfrag = np.cumsum([bsite_idx[i]>bsite_idx[i-1]+1 
                             for i in range(1,bsite_nres)])[-1]+1
    print("binding site size (NRES,NFRAG): %d %d"%(bsite_nres,bsite_nfrag))
    
    # dimensions of the hallucinated MSA
    NRES,NSEQ  = args.len,args.msa_num
    print("MSA size (NSEQ,NRES): %d %d"%(NSEQ,NRES))

    # don't apply dropout when hallucinating a single sequence
    if NSEQ==1:
        args.drop = 0.0
    
    # annealing parameters for bs imprinting
    beta_start = args.b0
    beta_shift = (args.b1-args.b0)/args.opt_iter
    
    ########################################################
    # 1. setup network
    ########################################################
    
    # load network weights
    weights = load_weights('/home/aivan/for/GyuRie/trRosetta2/training/models',params)

    # setup computation graph
    x = tf.placeholder(dtype=tf.float32, shape=(None,None,20))
    drop_rate = tf.placeholder(dtype=tf.float32, shape=())
    p2d,p3d = network(tf.pad(x,[[0,0],[0,0],[0,1]]),*weights,params,drop_rate)

    
    ########################################################
    # 2. generate background distributions
    ########################################################
    
    # background distributions are generated by passing 
    # multiple random MSAs through the trRosetta network
    # and averaging over the runs:
    NSEQ_BKG = 5    # number of random sequences in the MSA
    NRUN_BKG = 100  # number of runs

    # generate background distributions
    bkg_np = np.zeros((NRES,NRES,130))
    sess = tf.Session(config=config)
    print("generating background...")
    for i in range(NRUN_BKG):
        pssm = np.random.normal(0,0.1,size=(NSEQ_BKG,NRES,20))
        x_inp = argmax(pssm)
        p2d_ = sess.run(p2d, feed_dict={x:x_inp,drop_rate:0.0})
        bkg_np += np.mean(p2d_,axis=0)

    bkg_np /= NRUN_BKG

    print("bkg.shape=", bkg_np.shape)
    
    
    ########################################################
    # 3. hallucination
    ########################################################
    
    p2d = p2d[0]
    p3d = p3d[0]

    # placeholder for the inverse temperature (for bs imprinting)
    beta = tf.placeholder(dtype=tf.float32, shape=())

    # convert background probabilities to tf.tensor
    bkg_tf = tf.constant(bkg_np, dtype=tf.float32)

    # hallucination loss: KL-divergence of predicted probability 
    # distributions for dist,omega,theta,phi from background
    kl = -tf.math.reduce_sum(p2d*tf.math.log(p2d/bkg_tf),-1)
    kl = tf.math.reduce_mean(kl)/4

    # clash score: TODO - add comments
    p3dT = tf.transpose(p3d,[0,1,3,2])
    p3d_top2,_ = tf.nn.top_k(p3dT, k=2)
    #p1 = p3d_top2[:,:,:-1,0]
    #p2 = p3d_top2[:,:,:-1,1]
    clash = tf.reduce_sum((p3d_top2[:,:,:-1,0]*p3d_top2[:,:,:-1,1])**2)

    # probe b-site
    loss_sat,loss_consist,bsite_ij = probe_bsite_tf_frag(tf.expand_dims(p2d,axis=0),bsite,bsite_idx,beta,NRES)

    #losses = [kl,loss_sat[0],loss_consist[0],clash]
    #loss = kl + args.ws*loss_sat[0] + args.wc*loss_consist[0] + args.wcl*tf.nn.relu(clash-2.0)
    losses = [kl,loss_sat[0],loss_consist,clash]
    loss = kl + args.ws*loss_sat[0] + args.wc*loss_consist + args.wcl*tf.nn.relu(clash-2.0)

    # calculate gradient of the total loss 
    # with respect to the input MSA (x)
    grad = tf.keras.backend.gradients(loss, x)[0]

    # length-adjusted learning rate
    lr = args.step * np.sqrt(NRES)

    # fragment sizes and residue indices
    j = np.cumsum([0]+[bsite_idx[i]>bsite_idx[i-1]+1 for i in range(1,bsite_nres)])
    fs = np.array([np.sum(j==i) for i in range(j[-1]+1)])
    fi = [np.where(j==i)[0] for i in range(j[-1]+1)]

    # read checkpoint file if it exists
    f_checkpoint = f'{args.DIR}.chkp'
    if os.path.exists(f_checkpoint):
        with open(f_checkpoint, 'r') as f_in:
            hals_done = f_in.readlines()
            last_completed_hal = int(hals_done[-1]) if len(hals_done) != 0 else args.start_num - 1
            if last_completed_hal + 1 == args.start_num + args.num:
                print('All jobs have previously completed. There is nothing more to hallucinate.')
    else:
        last_completed_hal = args.start_num - 1

    # generate args.num designs
    for itr in range(last_completed_hal + 1, args.start_num+args.num):

        b=beta_start
        pssm = np.random.normal(0,0.1,size=(NSEQ,NRES,20))
        for k in range(1,args.opt_iter+1):

            if args.sample==True:
                x_inp = sample(pssm, args.seq_soft) 
            else:
                x_inp = argmax(pssm) 

            grad_,ls_,l_ = sess.run([grad,losses,loss], 
                                    feed_dict={x:x_inp, 
                                               drop_rate:args.drop, 
                                               beta:b})
            b+=beta_shift
            grad_ = grad_/np.linalg.norm(grad_, axis=(1,2), keepdims=True) + 1e-8
            pssm -= lr * grad_

            if k%10==0:
                print("\tstep %6s: [kl,sat,consist,clash]=[ %6.3f %6.3f %6.3f %6.3f ] total_loss=%.3f"%
                      ("%d/%d"%(itr,k),ls_[0],ls_[1],ls_[2],ls_[3],l_))

        # recalculate w/o dropout
        loss_,losses_,p2d_,p3d_,bs_ij_ = sess.run([loss,losses,p2d,p3d,bsite_ij], feed_dict={x:argmax(pssm), drop_rate:0.0, beta:b})

        # check binding site
        i2 = np.argsort(bs_ij_[0].flatten())[-(bsite_nfrag**2-bsite_nfrag):]
        G = nx.Graph()
        G.add_nodes_from([i for i in range(NRES)])
        G.add_edges_from([(i%NRES,i//NRES) for i in i2])
        max_clique_size = nx.algorithms.max_weight_clique(G,weight=None)[1]

        if max_clique_size > bsite_nfrag:
            max_clique_size = bsite_nfrag

        max_cliques = [c for c in nx.algorithms.enumerate_all_cliques(G) 
                       if len(c)==max_clique_size]

        mtx = np.sum(np.sum((p3d_[:,:,None,:,:-1]*
                             p3d_[:,:,:,None,:-1])**2,axis=-1),
                     axis=(0,1))
        np.fill_diagonal(mtx,0)

        print("bs size: %d out of %d"%(max_clique_size, bsite_nfrag), max_cliques)

        # enumerate all possible fragment orders and
        # identify the best scoring one
        trials = []
        for clique in max_cliques:
            for p in permutations(range(len(fs)),max_clique_size):
                p = np.array(p)
                a = np.hstack([np.arange(j)+i-(j-1)//2 for i,j in zip(clique,fs[p])])

                # skip if out of sequence range
                if np.sum(a<0)>0 or np.sum(a>=NRES)>0:
                    continue

                # skip if fragments clash
                if np.unique(a).shape[0]!=a.shape[0]:
                    continue

                b = np.hstack([fi[i] for i in p])

                P = p2d_[np.ix_(a,a.T)]
                Q = bsite[np.ix_(b,b.T)]
                s = np.mean(np.sum(-np.log(P)*Q,axis=-1))/4
                trials.append((s,a,b,p))

        trials.sort(key=lambda x: x[0])
        best=trials[0]
        zscore = stats.zscore([t[0] for t in trials])[0]
        print("best: score= %.5f   zscore= %.5f   trials= %d   order="%(best[0],zscore,len(trials)), best[3])
        sys.stdout.flush()

        seq = idx2aa(pssm[...,:20].argmax(-1)[0])
        #print(pssm[...,:-1].argmax(-1)[0])
        print(seq)


        if max_clique_size==bsite_nfrag:
            scores = np.hstack([ls_,[l_,np.sum(mtx),best[0],zscore]])
            name = "%s_%d.npz"%(args.DIR,itr)
            np.savez_compressed(name,
                                dist=p2d_[:,:,:37],
                                omega=p2d_[:,:,37:37*2],
                                theta=p2d_[:,:,37*2:37*3],
                                phi=p2d_[:,:,37*3:],
                                idx_pred=best[1],
                                idx_bsite=best[2],
                                msa=pssm[...,:20].argmax(-1),
                                p2d=bs_ij_,
                                scores=scores)

            name = "%s_%d.fas"%(args.DIR,itr)
            with open(name,'w') as f:
                f.write(">%s\n%s\n"%(name,seq))

            outdict = {}
            #outdict['con_ref_idx0'] = np.array([[bsite_idx[i] for i in best[2]]])
            outdict['con_ref_pdb_idx'] = np.array([('A',bsite_idx[i]) for i in best[2]])
            #outdict['con_hal_idx0'] = np.array([best[1]])
            outdict['con_hal_pdb_idx'] = np.array([('A',i+1) for i in best[1]])
            outdict['loss_nodrop'] = {'bkg':losses_[0], 'sat':losses_[1], 'consist':losses_[2],'clash':losses_[3], 'pdb':best[0]}
            outdict['settings'] = vars(args)
            with open('%s_%d.trb'%(args.DIR,itr), 'wb') as outf:
                pickle.dump(outdict, outf)

        # record completed design numbers in the checkpoint file
        with open(f_checkpoint, 'a+') as f_out:
            print(itr,file=f_out)

        
if __name__ == '__main__':
    main()

